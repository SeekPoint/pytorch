PyTorch 流水线并行实现 (6)--并行计算
https://www.cnblogs.com/rossiXYZ/p/15370731.html

0x00 摘要
前几篇文章我们介绍了 PyTorch 流水线并行的基本知识，自动平衡机制和切分数据，本文我们结合论文内容来看看如何实现流水线。

流水线并行其他文章链接如下:

[源码解析] 深度学习流水线并行Gpipe(1)---流水线基本实现

[源码解析] 深度学习流水线并行GPipe (2) ----- 梯度累积

[源码解析] 深度学习流水线并行 GPipe(3) ----重计算

[源码解析] 深度学习流水线并行之PipeDream(1)--- Profile阶段

[源码解析] 深度学习流水线并行 PipeDream(2)--- 计算分区

[源码解析] 深度学习流水线并行 PipeDream(3)--- 转换模型

[源码解析] 深度学习流水线并行 PipeDream(4)--- 运行时引擎

[源码解析] 深度学习流水线并行 PipeDream(5)--- 通信模块

[源码解析] 深度学习流水线并行 PipeDream(6)--- 1F1B策略

[源码解析] PyTorch 流水线并行实现 (1)--基础知识

[源码解析] PyTorch 流水线并行实现 (2)--如何划分模型

[源码解析] PyTorch 流水线并行实现 (3)--切分数据和运行时系统

[源码解析] PyTorch 流水线并行实现 (4)--前向计算

[源码解析] PyTorch 流水线并行实现 (5)--计算依赖

本文图片来自论文和github源码。

0x01 总体架构
我们首先从整体角度来梳理一下 torchgpipe。

..

0x02 并行拷贝和计算
我们接下来分析并行拷贝和计算（Concurrent Copy and Computation: Streams）。

2.1 GPU并行操作
我们首先看看 GPU 提供的并行操作功能。

CUDA流表示一个GPU操作队列，即某个设备绑定的，按照顺序执的核（kernel）序列。
我们可以把一个流看作是GPU之上的一个任务。用户向流的队列上添加一系列操作，GPU会按照添加到流中的先后顺序而依次执行这一系列操作。
在同一个流之中，所有操作是串行序列化，因此这些操作永远不会并行。
因此，要想并行，两个操作必须位于不同的 stream 中。
不同流中的核函数可以交错，甚至可能重叠。

几乎所有具有计算能力1.1及更高计算能力的CUDA设备都支持并发复制和执行，即设备重叠（Device Overlap）功能，其特点如下：

    1 数据拷贝和数值计算可以并行。

    2 两个方向的拷贝可以并行（GPU到CPU，CPU到GPU）。

    3 进行数值计算的kernel不能读写正在拷贝的数据。

因为 CPU 内存一般来说是大于 GPU内存，因此不可能把 CPU 内存一次性都拷贝到GPU，需要分块传输。
所以设备重叠功能就能够很好提高GPU程序的执行效率，比如：

    1 将数据拆分成为许多块，每一块交给一个Stream来处理。

    2 每一个Stream会进行如下操作：
        1 将属于该Stream的数据从host内存拷贝进device内存，

        2 GPU进行 kernel 运算，将计算结果保存到GPU内存，

        3 把 Stream计算结果从device 内存拷贝回host内存。

    3 GPU的scheduler决定 stream 如何并行。

    4 CPU 的操作也可以同时并行。

2.2 PyTorch
除非另有指定，PyTorch将每个绑定到设备的核函数发布到默认流。
因为前向传播位于 default stream 中，所以要想并行处理 "下一个 batch 数据的预读取（拷贝CPU到GPU）" 和 "当前 batch 的前向传播"，就必须做到：

    cpu 上的 batch 数据 必须是 pinned。
    锁页可以使得硬件设备直接访问CPU内存，这样就减少了某些复制操作，锁定的页面不可以被交换到硬盘之上。
    在GPU上分配的内存默认都是锁页内存。

    预读取操作必须在另一个 stream 上进行。

Torchgpipe将每个拷贝核注册到非默认流中，同时将计算核保留在默认流中。 yknote接下来这句话用图 01-34 表示

此外，每个device对每个微批次使用不同的流。由于不同的微批次之间没有真正的依赖关系，因此流的这种使用是安全的，这允许尽可能快地进行拷贝。
请参见下图。
    01-35.png
图上表示的是设备 j 的时间线，是否使用非默认流进行复制

    (a）部分的意思是：仅使用默认流，复制核可能会阻塞计算核（反之亦然），直到复制完全完成。

    (b）部分的意思是：使用复制流，计算可以与从其他设备发送或接收数据同时进行。

2.3 Stream 封装
因为是对stream进行操作，所以 torchgpipe 对底层流操作进行了一些基础封装，
流相关主要代码位于：torchgpipe/stream.py。

...


0x03 重计算
我们接下来看看重计算，在论文中是 Autograd Functions with Shared Memory 这节。

因为之前在 GPipe 之中我们介绍过类似部分，所以这里只是为了行文完整性而加入，故此分析较略。

3.1 解析  yknote  此段落由图 01-37.ng

3.4 总体调用
总体调用代码如下：

    def compute(self,
                schedule: List[Tuple[int, int]],
                ----yknoe多次反复提到

至此，PyTorch 流水线并行分析完毕，我们接下来的计划是把PyTorch 并行训练再系统梳理一下，首先需要分析其梯度相关基础知识，敬请期待。

0xFF 参考
Markdown公式用法大全

markdown中公式编辑教程

https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html#stream-sync-behavior

CUDA学习：基础知识小结

CUDA随笔之Stream的使用

NVIDIA解决方案架构师深度解析大规模参数语言模型Megatron-BERT

Accelerating Wide & Deep Recommender Inference on GPUs

HugeCTR: High-Performance Click-Through Rate Estimation Training

https://discuss.pytorch.org/t/how-to-prefetch-data-when-processing-with-gpu/548

https://github.com/NVIDIA/apex/

https://github.com/justheuristic/prefetch_generator

https://pytorch.org/tutorials/intermediate/model_parallel_turotial.html

https://pytorch.org/docs/stable/autograd.html

https://pytorch.org/docs/notes/cuda.html

https://zhuanlan.zhihu.com/p/61765561

https://pytorch.apachen.org/docs/1.7/64.html

https://zhidx.com/p/217999.html


评论列表

大神，能把Recompute那一块再往细了讲一讲么？完全不明白为啥分成两个autograd函数后，copy数据和F_{i,j}'就可以并行了。原来的Pytorch原生的就不能并行么？

还有一个问题，B_{i+1,j}为什么必须在B_{i,j}之前执行，感觉第i+1个micro-batch和第i个micro-batch之间的计算互相不影响啊。我先执行B_{i,j}再执行B_{i+1,j}不行么？

支持(0) 反对(0)
   回复 引用#2楼 [楼主] 2022-03-12 19:00 罗西的思考
@梦幻超级佳

你好，

关于第一个问题，https://www.cnblogs.com/rossiXYZ/p/15172960.html 4.3 节有比较详细的说明，基本思路就是 CheckpointFunction 这个过程可以继续切分，可以做进一步的计算和通信的overlap，可以更好的利用流水线。

关于第二个问题，论文之中做了设定，In addition to this, it is enforced that Fi,j must be completed before executing Fi+1,j and Bi,j must be completed before executing Bi 1,j。
如果像你说的那样并行应该也可以，但是那样在具体开发过程之中可能会难以调试难以保证正确性。而对设备之间的依赖做限定可以确保正确性。

如果还有问题，或者发现错误，请留言。